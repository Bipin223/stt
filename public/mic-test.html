<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Microphone Test - Vercel</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 5px;
            cursor: pointer;
            margin: 10px;
            font-size: 16px;
        }
        button:hover {
            background: #0056b3;
        }
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        .status {
            margin: 20px 0;
            padding: 15px;
            border-radius: 5px;
            font-weight: bold;
        }
        .success { background: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
        .error { background: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }
        .info { background: #d1ecf1; color: #0c5460; border: 1px solid #bee5eb; }
        .debug {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            font-family: monospace;
            white-space: pre-wrap;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸŽ¤ Microphone & Speech Recognition Test</h1>
        <p>This page helps debug microphone and speech recognition issues on Vercel deployment.</p>
        
        <div id="status" class="status info">Ready to test...</div>
        
        <button onclick="testMicrophone()">Test Microphone Access</button>
        <button onclick="testSpeechRecognition()">Test Speech Recognition</button>
        <button onclick="testBoth()">Test Both</button>
        
        <div id="debug" class="debug"></div>
        
        <div id="transcript" style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 5px; min-height: 100px;">
            <strong>Transcript will appear here...</strong>
        </div>
    </div>

    <script>
        const statusEl = document.getElementById('status');
        const debugEl = document.getElementById('debug');
        const transcriptEl = document.getElementById('transcript');
        
        function updateStatus(message, type = 'info') {
            statusEl.textContent = message;
            statusEl.className = `status ${type}`;
        }
        
        function addDebug(message) {
            debugEl.textContent += new Date().toLocaleTimeString() + ': ' + message + '\n';
        }
        
        // Initial environment check
        window.addEventListener('load', () => {
            const info = {
                protocol: location.protocol,
                hostname: location.hostname,
                userAgent: navigator.userAgent,
                hasGetUserMedia: !!navigator.mediaDevices?.getUserMedia,
                hasSpeechRecognition: !!(window.SpeechRecognition || window.webkitSpeechRecognition),
                isSecureContext: window.isSecureContext
            };
            
            addDebug('Environment Info: ' + JSON.stringify(info, null, 2));
            
            if (!info.hasGetUserMedia) {
                updateStatus('âŒ getUserMedia not available', 'error');
            } else if (!info.hasSpeechRecognition) {
                updateStatus('âŒ Speech Recognition not available', 'error');
            } else if (!info.isSecureContext) {
                updateStatus('âŒ Not in secure context (HTTPS required)', 'error');
            } else {
                updateStatus('âœ… All APIs available', 'success');
            }
        });
        
        async function testMicrophone() {
            updateStatus('ðŸŽ¤ Testing microphone access...', 'info');
            addDebug('Testing microphone access...');
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                addDebug('âœ… Microphone access granted');
                updateStatus('âœ… Microphone access successful', 'success');
                
                // Stop the stream
                stream.getTracks().forEach(track => track.stop());
                
                return true;
            } catch (error) {
                addDebug('âŒ Microphone error: ' + error.message);
                updateStatus('âŒ Microphone access failed: ' + error.message, 'error');
                return false;
            }
        }
        
        function testSpeechRecognition() {
            updateStatus('ðŸ—£ï¸ Testing speech recognition...', 'info');
            addDebug('Testing speech recognition...');
            
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            
            if (!SpeechRecognition) {
                addDebug('âŒ Speech Recognition not available');
                updateStatus('âŒ Speech Recognition not supported', 'error');
                return false;
            }
            
            const recognition = new SpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            
            recognition.onstart = () => {
                addDebug('âœ… Speech recognition started');
                updateStatus('ðŸŽ¤ Listening... Speak now!', 'success');
            };
            
            recognition.onresult = (event) => {
                let transcript = '';
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    transcript += event.results[i][0].transcript;
                }
                transcriptEl.innerHTML = '<strong>Transcript:</strong><br>' + transcript;
                addDebug('ðŸ“ Transcript: ' + transcript);
            };
            
            recognition.onerror = (event) => {
                addDebug('âŒ Speech recognition error: ' + event.error);
                updateStatus('âŒ Speech recognition error: ' + event.error, 'error');
            };
            
            recognition.onend = () => {
                addDebug('ðŸ›‘ Speech recognition ended');
                updateStatus('Speech recognition stopped', 'info');
            };
            
            try {
                recognition.start();
                addDebug('Starting speech recognition...');
                
                // Auto-stop after 10 seconds
                setTimeout(() => {
                    try {
                        recognition.stop();
                    } catch (e) {
                        addDebug('Error stopping recognition: ' + e.message);
                    }
                }, 10000);
                
                return true;
            } catch (error) {
                addDebug('âŒ Failed to start speech recognition: ' + error.message);
                updateStatus('âŒ Failed to start speech recognition: ' + error.message, 'error');
                return false;
            }
        }
        
        async function testBoth() {
            const micResult = await testMicrophone();
            if (micResult) {
                setTimeout(() => {
                    testSpeechRecognition();
                }, 1000);
            }
        }
    </script>
</body>
</html>